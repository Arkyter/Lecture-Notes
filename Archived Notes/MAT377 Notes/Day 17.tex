\section{Day 17: Distributions Related to Gaussian (Nov. 11, 2024)}
This is \textit{Section 4.4} in Panchenko! Recall that the $\Gamma$ function is given by
\[ \Gamma(\alpha) = \int_0^\infty x^{\alpha - 1} e^{-x} \, dx; \]
if we divide both sides by $\Gamma(\alpha)$ and perform a change of variables $x = \beta y$ for $\beta \geq 0$, we get
\[ 1 = \int_0^{\infty} \frac{1}{\Gamma(\alpha)} x^{\alpha - 1}e^{-x} \, dx = \int_0^{\infty} \frac{\beta^\alpha}{\Gamma(\alpha)} y^{\alpha - 1} e^{-\beta y} \, dy; \]
thus, we see that
\[ f_{\alpha, \beta}(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{- \beta x} 1_{\{x \geq 0\}} \]
for each $\alpha, \beta > 0$ is a density, and is called the \textit{Gamma distribution with parameters} $\alpha, \beta$, and is written $\Gamma(\alpha, \beta)$. Let $X_i \sim \Gamma(\alpha_i, \beta)$ be independent; then we have $X_1 + \dots + X_n \sim \Gamma(\alpha_1 + \dots + \alpha_n, \beta)$. If $X, Y$ are independent with densities $f, g$, then $X + Y$ has density
\[ (f \ast g)(x) = \int_{-\infty}^\infty f(x - y)g(y) \, dy. \]
We use these two properties to inductively prove that the sum is indeed distributed $\Gamma(\alpha_1 + \dots + \alpha_n, \beta)$; for $n = 2$, we have (from convolution directly),
\begin{align*}
    & \int_0^x \frac{\beta^{\alpha_1 + \alpha_2}}{\Gamma(\alpha_1) \Gamma(\alpha_2)} e^{-\beta(x - y)} e^{- \beta y} (x - y)^{\alpha_1 - 1} y^{\alpha_2 - 1} \, dy \\
    &= \frac{\beta^{\alpha_1 + \alpha_2}}{\Gamma(\alpha_1) \Gamma(\alpha_2)} e^{-\beta x} \int_0^x (x - y)^{\alpha_1 - 1} y^{\alpha_2 - 1} \, dy \tag{Substitute $y = xz$} \\
    &= \frac{\beta^{\alpha_1 + \alpha_2}}{\Gamma(\alpha_1) \Gamma(\alpha_2)} e^{-\beta x} \int_0^1 (x - xz)^{\alpha_1 - 1} xz^{\alpha_2 - 1} x \, dz \\
    &= \frac{\beta^{\alpha_1 + \alpha_2}}{\Gamma(\alpha_1) \Gamma(\alpha_2)} e^{-\beta x} x^{\alpha_1 + \alpha_2 - 1} \int_0^1 (1 - z)^{\alpha_1 - 1} z^{\alpha_2 - 1} \, dz \\
    &= \beta^{\alpha_1 + \alpha_2} e^{-\beta x} x^{\alpha_1 + \alpha_2 - 1} C 1_{\{x \geq 0\}},
\end{align*}
where
\[ C = \frac{\int_0^1 (1 - z)^{\alpha_1 - 1} z^{\alpha_2 - 1} \, dz}{\Gamma(\alpha_1) \Gamma(\alpha_2)} = \frac{1}{\Gamma(\alpha_1 + \alpha_2)}. \]
We see that this just means that by induction, we get a $\Gamma(\alpha_1 + \dots + \alpha_n, \beta)$ distribution as desired. Now, if $g_1, \dots, g_n$ are independent standard Gaussians, then $g_1^2 + \dots + g_n^2 \sim \chi_n^2$, i.e. ``chi squared with $n$ degrees of freedom'', where
\[ \chi_1^2 \sim \frac{1}{\sqrt{2n}} x^{\frac{1}{2} - 1} e^{-\frac{1}{2}} 1_{\{x \geq 0\}} \sim \Gamma\left(\frac{1}{2}, \frac{1}{2}\right). \]
We know that $\chi_n^2 \sim \Gamma(\frac{n}{2}, \frac{1}{2})$; if $X \sim \chi_k^2$, $Y \sim \chi_m^2$, then the distribution of the ratio is given by $Z = \frac{X/k}{Y/m} \sim F_{k, m}$, i.e. the ``$F$ distribution with degrees of freedom $k, m$''.
\begin{simplelemma}
    If $X, Y > 0$ and independent with densities $f, g$ then $\frac{X}{Y}$ has density $\int_0^\infty f(xy)g(y) y \, dy$.
\end{simplelemma}
\noindent Write
\begin{align*}
    \PP \left( \frac{X}{Y} \leq t \right) = \PP(X \leq tY) &= \int_0^\infty \PP(X \leq ty) g(y) \, dy \\
    &= \int_0^\infty \int_0^{ty} f(x) g(y) \, dx \, dy \tag{Substitute $x = zy$} \\
    &= \int_0^\infty \int_0^t f(zy) g(y) y \, dz \, dy \tag{Fubini} \\
    &= \int_0^t \left(\int_0^\infty f(zy) g(y) y \, dy \right) \, dz
\end{align*}
as desired. Now, write $X \sim \chi_n^2$, $Y \sim \chi_m^2$, and $f_{\chi_k^2} (xy)$. We have
\begin{align*}
    f_{\frac{X}{Y}}(x) &= \int_0^\infty \underbrace{\frac{\left(\frac{1}{2}\right)^\frac{k}{2}}{\Gamma(\frac{k}{2})} (xy)^{\frac{k}{2} - 1} e^{-\frac{1}{2}xy}}_{f_{\chi_k^2 (xy)}} \underbrace{\frac{\left(\frac{1}{2}\right)^\frac{m}{2}}{\Gamma(\frac{m}{2})} (xy)^{\frac{m}{2} - 1} e^{-\frac{1}{2}y}}_{y_{\chi_m^2}(y)} y \, dy \\
    &= \frac{\left(\frac{1}{2}\right)^{\frac{k+m}{2}}}{\Gamma(\frac{k}{2}) \Gamma(\frac{m}{2})} x^{\frac{k}{2} - 1} \int_0^\infty y^{\frac{x+k}{2} - 1} e^{-\frac{1}{2}(x + 1) y} \, dy \tag{$z = \frac{1}{2}(x + 1)y$} \\
    &= \frac{\Gamma(\frac{k+m}{k})}{\Gamma{\frac{k}{2}} \Gamma(\frac{m}{2}) } x^{\frac{k}{2} - 1} (1 + x)^{- \frac{k + m}{2}}.
\end{align*}
In particular, this means
\[ f_{k, m}(x) = \frac{\Gamma(\frac{k+m}{2})}{\Gamma(\frac{k}{2}) \Gamma(\frac{k}{2}) } k^{\frac{k}{2}} m^{\frac{m}{2}} x^{\frac{k}{2} - 1} (m + kx)^{-\frac{k+m}{2}}. \]
Now, let $g_0, \dots, g_n$ be independent standard Gaussians. Then the distribution of
\[ T = \frac{g_0}{\sqrt{\frac{1}{n} (g_1^2 + \dots + g_n^2) }} \]
is the Student's $T$-distribution with $n$ degrees of freedom, often written $t_n$. Writing
\[ T^2 = \frac{g_0^2}{\frac{1}{n}(g_1^2 + \dots + g_n^2)} \sim F_{1, n}, \]
we have that $\PP(T^2 \leq t^2) = \PP(-t < T < t) = 2 \PP(0 \leq T \leq t)$ by symmetry; we may write
\[ 2 \int_0^t f_T(x) \, dx = \int_0^{t^2} f_{1, n}(x) \, dx = \int_0^t f_{1, n} (y^2) 2y \, dy, \]
so
\[ f_T(t) = f_{1, n}(t^2) t = \frac{\Gamma(\frac{n+1}{2})}{\Gamma(\frac{1}{2}) \Gamma(\frac{n}{2})} \frac{1}{\sqrt{n}} \left(1 + \frac{t^2}{n}\right)^{- \frac{n + 1}{2}}. \]
As $n \to \infty$, we get that
\[ \left(1 + \frac{t^2}{2 \left(\frac{n}{2}\right)}\right)^{-\frac{n}{2} + \frac{1}{2}} \xrightarrow[]{n \to \infty} e^{-\frac{t^2}{2}}, \]
and
\[ \frac{\Gamma(\frac{n+1}{2})}{\Gamma(\frac{n}{2})} \frac{1}{\sqrt{n}} \xrightarrow[]{n \to \infty} \frac{1}{\sqrt{2 \pi}}, \]
so we have
\[ \lim_{n \to \infty} f_T(t) = \frac{1}{\sqrt{2\pi}} e^{-t^2 / 2}. \]

\newpage
\noindent We now move onto linear regressions (section 4.5 in Panchenko). Let $(x_1, y_1), \dots, (x_n, y_n)$ be data points; the simple linear regression (SLR) model is $Y_i = \beta_0 + \beta_1 X_i^2 + \eps_i$, where $X_i$ are independent variables, $y = f(x) = \beta_0 + \beta_1 x$ is the regression line, and $\eps_i$ are Gaussian distributed $N(0, \sigma^2)$. The density of $\vec{y}$ is given by
\[ \ell_{\beta_0, \beta_1, \sigma} = \prod_{i=1}^n \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{1}{2 \sigma^2} (y - \beta_0 - \beta_i X_i)^2}, \]
where $\ell$ represents the \textit{likelihood function}. We may write
\[ \max_{\beta_0, \beta_1, \sigma} \ell_{(\beta_0, \beta_1, 0, (x_1, \dots, x_n, y_1, \dots, y_n))} \]
as the maximum likelihood estimate. We start by maximizing over $\beta_0, \beta_1$. Now, we just need to minimize $L = \sum_{i=1}^n (y_i - \beta_0 - \beta_i x_i)^2$. We have
\begin{align*}
    \frac{\partial L}{\partial \beta_0} &= -2 \sum_{i=1}^n Y_i - (\beta_0 + \beta_1 X_i) = 0,
    \frac{\partial L}{\partial \beta_1} &= -2 \sum_{i=1}^n Y_i - (\beta_0 + \beta_1 X_i) X_i = 0.
\end{align*}
Solving the above, we have
\[ \hat{\beta_0} := \overline{Y} - \hat{\beta_1} \overline{X}, \hspace{0.3in} \hat{\beta_1} := \frac{\overline{XY} - \overline{X} \overline{Y}}{\overline{X^2} - \overline{X}^2}, \]
where $\overline{X}, \overline{Y}, \overline{XY}$ are given by $\frac{1}{n} \sum_{i=1}^n X_i$, $\frac{1}{n} \sum_{i=1}^n Y_i$, and $\frac{1}{n} \sum_{i=1}^n X_i Y_i$ respectively. Now, we want to maximize
\[ - n \log \sigma - \frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \hat{\beta_0} - \hat{\beta_1} x_i)^2 \]
over $\sigma$. A few more things that I just didn't record cuz too tired. x3