\section{Day 3: Distributions, Stability Property, Moments (Sep. 11, 2024)}
Recall the Poisson distribution
\[ P(X = n) = \frac{\lambda^n}{n!}e^{-\lambda} \]
where $n = 0, 1, \dots$ and $\lambda > 0$. We introduce the stability property: let us consider the independent random variables $X_1, X_2$ distributed as follows,
\begin{align*}
    X_1 &\sim \mathrm{Poiss}_{\lambda_1}, \\
    X_2 &\sim \mathrm{Poiss}_{\lambda_2}.
\end{align*}
Then we have that $X_1 + X_2 \sim \mathrm{Poiss}_{\lambda_1 + \lambda_2}$. To prove this, write
\begin{align*}
    P(X_1 + X_2 = n) &= \sum_{m=0}^n P(X_1 = m, X_2 = n-m) \\
    &= \sum_{m=0}^n P(X_1 = m) P(X_2 = n-m) \\
    &= \sum_{m=0}^n \frac{\lambda_1^m}{m!} e^{-\lambda_1} \frac{\lambda_2^{m-2}}{(m-n)!} e^{-\lambda_2} \\
    &= \frac{1}{n!} \underbrace{\sum_{m=0}^n \frac{n!}{m!(n - m)!} \lambda_1^m \lambda_2^{n-m}}_{(\lambda_1 + \lambda_2)^n} e^{-(\lambda_1 + \lambda_2)} \\
    &= \frac{1}{n!} (\lambda_1 + \lambda_2)^n e^{-(\lambda_1 + \lambda_2)}.
\end{align*}
Binomials also have a related property; let
\begin{align*}
    X_1 &\sim \mathrm{Bin}(n_1, p), \\
    X_2 &\sim \mathrm{Bin}(n_2, p).
\end{align*}
Given that $X_1, X_2$ are independent, we know that $X_1 + X_2 \sim \mathrm{Bin}(n_1 + n_2, p)$. To prove this, we may just write
\begin{align*}
    X_1 &= y_1 + \dots + y_{n_1}, \\
    X_2 &= y_{n_1 + 1} + \dots + y_{n_1 + n_2}, \\
    X_1 + X_2 &= y_1 + \dots + y_{n_1 + n_2} \sim \mathrm{Bin}(n_1 + n_2, p).
\end{align*}
Moreover, we also have $\mathrm{Bin}(n, \frac{\lambda}{n}) \stackrel{n \to \infty}{\to} \mathrm{Poiss}_\lambda$. This is called the \textit{law of little numbers}. To prove this, we have
\begin{align*}
    \binom{n}{k} \left(\frac{\lambda}{n}\right)^k \left(1 - \frac{\lambda}{n}\right)^{n-k} &= \frac{\lambda^k}{k!} \underbrace{\frac{n (n-1) \dots (n - k + 1)}{n^k}}_{\to 1} \underbrace{\left(1 - \frac{\lambda}{n}\right)^{n-k}}_{\to e^{-\lambda}} \\
    &\stackrel{n \to \infty}{\to} \frac{\lambda^k}{k!} e^{-\lambda} = \mathrm{Poiss}_\lambda.
\end{align*}

\newpage
\noindent There are two examples of Poisson distributions that we will go over: shark attacks and radioactive decay. (but we didn't go over it ig?)
\begin{simplethm}[Doeblin]
    Let $X_i$ be independent random variables distributed by $\mathrm{Ber}_{p_i}$, where $0 < p_i < 1$. Let us have $S_n = X_1 + \dots + X_n$ with $\lambda = p_1 + \dots + p_n$. Then
    \[ \abs{P(S_n \in A) - \sum_{n \in A} \frac{\lambda^n}{n!} e^{-\lambda}} \leq \sum_{i=1}^n p_i^2, \]
    where $A \subset \{0, 1, \dots\}$.
\end{simplethm}
\noindent To prove this, let $y$ be a random variable where $y \sim \mathrm{Poiss}_p$, then $P(y = 0) = e^{-p} > 1 - p$. Define $\Omega^{\perp} = \{-1, 0, 1, 2, \dots\}$; then we have $P_p(-1) = 1 - p, P_p(0) = e^{-p} - 1 + p, P_p(k) = \frac{p^k}{k!} e^{-p}$ for $k = 1, 2, 3, \dots$. Moreover, define
\[ X(\omega) = \begin{cases} 0 & \omega = -1 \\ 1 & \omega \geq 0 \end{cases}, \hspace{0.2in} y(\omega) = \begin{cases} 0 & \omega = 0, 1 \\ \omega & \omega \geq 1 \end{cases}. \]
Then $P(x = y) = 1 - p + pe^{-p} \geq 1 - p + p(1 - p) = 1 - p^2$, so $P(x = y) \leq p^2$. Take $\Omega = (\Omega_+)^n$ and $X_i(\omega) = X(\omega)$, and let us have
\[ P(\omega) = \prod_{i=1}^n P_{p_i} (\omega_i) \]
where $X_i$ are independently distributed by $\mathrm{Ber}_p$, and $y_i$ are independently distributed by $\mathrm{Poiss}_{p_i}$. Finally, let us have
\begin{align*}
    S_n &= X_1 + \dots + X_n, \\
    S_n' &= y_1 + \dots + y_n,
\end{align*}
then $P(S_n \neq S_n') \leq \sum_{i=1}^n P(X_i \neq y_i) \leq \sum_{i=1}^n p_i^2$, which means $S_n' \sim \mathrm{Poiss}_\lambda$.\footnote{reminder: review this proof, i'm stupid and don't really get it.} \qed
\bigskip\hrule\bigskip
\noindent We now define \textit{moments}. For a random variable $X$, $E(X) = \sum_{\omega \in \Omega} X(\omega) P(\omega)$, as long as $E\abs{X} < \infty$. Then moments are given by $EX^n$ where $n = 1, 2, \dots.$ (??) For example, let $X = \mathrm{Poiss}_\lambda$. Then we have for $n = 1$,
\begin{align*}
    EX &= \sum_{n=0}^\infty n \frac{\lambda^n}{n!} e^{-\lambda} \\
    &= \sum_{n=0}^\infty \frac{\lambda^n}{(n - 1)!} e^{-\lambda} \\
    &= \sum_{n=0}^\infty \frac{\lambda^{n+1}}{n!} e^{-\lambda} \\
    &= \lambda.
\end{align*}
For $n = 2$, we have
\begin{align*}
    EX^2 &= \sum_{n=0}^\infty n^2 \frac{\lambda^n}{n!} e^{-\lambda} \\
    &= E(X(X - 1)) + EX \\
    &= \lambda^2 + \lambda.
\end{align*}