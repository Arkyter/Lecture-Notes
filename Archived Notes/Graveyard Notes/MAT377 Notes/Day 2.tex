\section{Day 2: Expectations and Distributions (Sep. 9, 2024)}
Course administrative details first; starting next week, office hours will be held on Monday from 11:15am to 12:15pm. Recap of last lecture:
\begin{itemize}
    \item A probability space $\Omega$ is the set of all possible outcomes of an ``experiment,'' i.e. a countable set of individual events $\{\omega_1, \dots, \omega_n\}$ (we will cover continuous probability later on).
    \item $\SF = \SP(\Omega)$ is the set of all subsets of $\Omega$.
    \item $P(A) = \sum_{\omega \in \Omega} P(\omega)$ is the probability of an outcome in $A \in \SF$ occurring.
\end{itemize}
\hrule \bigskip
\noindent A random variable $X$ is a function $\Omega \to \RR$, aka the measurement of the event, and the expectation of the random variable, $EX$, is given by $\sum_{\omega \in \Omega} X(\omega) P(\omega)$. Is expectation well behaved? No. For example, consider the St. Petersburg Paradox; suppose you are playing a game in the casino; every time you flip a coin, your prize money doubles if it lands on heads (read: double or nothing lfg!!!). Then we may consider the set of outcomes to be the number of consecutive heads, i.e.
\begin{align*}
    \Omega &= \{1, 2, 3, \dots\}, \\
    P(n) &= \frac{1}{2^n}, \\
    X(n) &= 2^n.
\end{align*}
Clearly, the chance of getting $n$ heads in a row is $2^{-n}$, and assuming your prize money started at $1$ dollar, you would win $2^n$ dollars for said $n$ heads. Taking the expectation of this game, we find
\[ EX = \sum_{n=1}^{\infty} 2^n \cdot \frac{1}{2^n} = \sum 1 = \infty. \]
It doesn't make sense to expect to win infinite amounts of money from this game\footnote{martingale strat lfg.,..} unless you had unlimited wealth to start with. With this in mind, we insist on
\[ \sum_{\omega \in \Omega} \abs{X(\omega)} P(\omega) < \infty \]
within the context of this class.
\begin{simplethm}[Linearity of Expectation]
    $X$ is linear; i.e., $E[ax + by] = aEx + bEy$.
\end{simplethm}
We start with a lemma:
\begin{simplelemma}
    Let us have a bijective map $\pi : \NN \times \NN \to \NN$. Then
    \[ \sum_{n=1}^\infty c_n = \sum_{n=1}^\infty \sum_{m=1}^\infty C_{\pi(n, m)} \]
    if all $c_n \geq 0$ or if either side is absolutely convergent.
\end{simplelemma}

\newpage
\noindent We proceed to prove this with casework.
\begin{itemize}
    \item Suppose $c_n \geq 0$; then using the bijective nature of $\pi$, we may choose large enough $N, M$ such that
    \[ \sum_{n=1}^K c_n \leq \sum_{n=1}^N \sum_{m=1}^M c_{\pi(n, m)} \]
    for any choice of $K$. Conversely, we may pick
    \[ \sum_{n=1}^N \sum_{m=1}^M c_{\pi(n, m)} \leq \sum_{n=1}^K c_n \]
    for any $N, M$ by picking $k \geq \max_{\substack{1 \leq n \leq N \\ 1 \leq m \leq M}}\{\pi(n, m)\}$.
    Now, let $M \to \infty$; we have
    \[ \sum_{n=1}^N \sum_{m=1}^\infty c_{\pi(n, m)} \leq \sum_{n=1}^\infty c_n, \]
    then let $N \to \infty$ to get
    \[ \sum_{n=1}^\infty \sum_{m=1}^\infty c_{\pi(n, m)} \leq \sum_{n=1}^\infty c_n. \]
    As per earlier, we also see that LHS is greater or equal to RHS, which implies equality. \qed

    \item Now, suppose $\sum_{n=1}^\infty \abs{c_n} < \infty$. Let $c_n = a_n - b_n$, where $a_n = c_n 1(c_n \geq 0)$ and $b_n = c_n 1(c_n < 0)$. Then we obtain
    \[ \sum_{n=1}^\infty a_n = \sum_{n=1}^\infty \sum_{m=1}^\infty a_\pi(n, m), \hspace{0.2in} \sum_{n=1}^\infty b_n = \sum_{n=1}^\infty \sum_{m=1}^\infty b_\pi(n, m) \]
    as per our proof above. Summing both, we conclude that equality holds for absolute convergence as well. \qed
\end{itemize}

\noindent For now, let $X$ take values $\{a_1, a_2, \dots\}$ (countably many). Consider
\[ P'(a_n) = P(X = a_n) = P(\underbrace{\{\omega \mid X(\omega) = a_n\}}_{X^{-1}(a_n)}) \]
as the probably of a pre-image (or, $P' = P \circ X$). We see that $P'$ is a probability on $\RR$ (concentrated on $\{a_1, a_2, \dots\}$), and $0 \leq P'(a_n) \leq 1$ for any $n$; from now, we will call $P'$ a \textit{distribution} of $X$. Here are some examples of distributions:
\begin{itemize}
    \item The Bernoulli distribution: let $0 \leq p \leq 1$. Then consider a coin with $p$ chance to land on heads, and $1-p$ on tails; then $Ber_p$ is given by $\Omega = \{H, T\}$,
    \begin{align*}
        X(H) &= 1, P(X = 1) = p, \\
        X(T) &= 0, P(X = 0) = 1-p.
    \end{align*}
    \item Flip $N$ coins, with $X = \{0, 1, \dots, N\}$ being the number of heads we obtain. Then
    \[ P(X = \ell) = \binom{N}{\ell} p^\ell (1 - p)^{N - \ell}, \]
    and the expected value is given by
    \[ EX = \sum_{\ell = 0}^N \ell \binom{N}{\ell} p^\ell (1 - p)^{N - \ell}. \]
    Using linearity of expectation, we see $EX = EX_1 + \dots + EX_n = Np$ by separating each coinflip.
\end{itemize}
Expectation enjoys the change of variables property;\footnote{read: sum of value of outcome multiplied by the chance it occurs over all $\omega$ is the same as going over each value individually and multiplying the chance you roll into it}
\[ EX = \sum_{\omega \in \Omega} X(\omega) P(\omega) = \sum_{n=1}^\infty a_n P'(a_n). \]
To see this, consider partioning the probability space $\Omega$ into $X^{-1} = \{\omega_{nm} \mid 1 \leq m \leq M_n\}$ in terms of their measurement from $X$ (where $X(\omega_{ni}) = X(\omega_{nj}) = a_n$ for any $1 \leq i, j \leq M_n$)\footnote{read $M_n$ as a counter of how many outcomes in $\Omega$ have the same measurement of $a_n$}, and write
\[ \sum_{\omega \in \Omega} X(\omega) P(\omega) = \sum_{n=1}^\infty \sum_{m=1}^{M_n} X(\omega_{nm}) P(\omega_{nm}) \tag{by Lemma}, \]
where we may note that mapping each individual $\omega \in \Omega$ to some index $nm$ is bijective since it is a partition. We continue by writing
\begin{align*}
    &= \sum_{n=1}^\infty \sum_{m=1}^{M_n} a_n P(\omega_{nm}) = \sum_{n=1}^\infty a_n \left(\sum_{m=1}^{M_n}  P(\omega_{nm})\right) \\
    &= \sum_{n=1}^\infty a_n P(X = a_n),
\end{align*}
where we may note $P(X = a_n) = P'(a_n)$. \qed
\medskip\newline
\noindent The probability distribution of any given random variable $X$ also approaches $0$ at its tail. Specifically, we have that $\lim_{t \to \infty} P(x \geq t) = 0$. To prove this, we start by observing that $P(x \geq t)$ is monotone decreasing; consider
\[ P(X \geq n) = \sum_{m = n}^\infty P(m \leq X < m+1). \]
Clearly, the sum is convergent, as the sum of probabilities is equal to $1$. Using the fact that the tail of a convergent series approaches $0$, we conclude that $P(m \leq X < m + 1) \to 0$ as $m \to \infty$, and so $P(X \geq n) \to 0$ as $n \to \infty$.

\newpage
\begin{simplelemma}[Expectation of Random Variable in terms of Integral]
    The expectation of a random variable $X$ may be expressed as $EX = \int_0^\infty P(X \geq t) \, dt$ for $X \geq 0$.\footnote{intuition: layer cake formula, but compile them together in level sets.}
\end{simplelemma}
\noindent Let us start by considering the case where $X$ takes integer values only;
\begin{align*}
    EX = \sum_{n=1}^\infty n P(X = n) &= \sum_{n=1}^\infty \sum_{m=1}^n P(X = n) \\
    &= \sum_{m=1}^\infty \sum_{n=m}^\infty P(X = n) \\
    &= \sum_{m=1}^\infty P(X \geq m).
\end{align*}
For the general case, let us start by writing $a_n = \int_0^\infty 1(t \leq a_n) \, dt$ by the layer cake decomposition. Then
\begin{align*}
    EX = \sum_{n=1}^\infty a_n P(X = a_n) &= \sum_{n=1}^\infty \left(\int_0^\infty 1(t \leq a_n) \, dt\right) P(X = a_n) \\
    &\stackrel{(\ast)}{=} \int_0^\infty \left(\sum_{n=1}^\infty 1(t \leq a_n) P(X = a_n)\right) \, dt \tag{Fubini} \\
    &= \int_0^\infty \sum_{a_n \geq t} P(X = a_n) \, dt \\
    &= \int_0^\infty P(X \geq t) \, dt
\end{align*}
To resolve $(\ast)$ without the use of Fubini's theorem, we may write
\begin{align*}
    \sum_{n=1}^\infty \int_0^\infty 1(t \leq a_n) P(X < a_n) \, dt &= \sum_{n=1}^\infty \sum_{m=1}^\infty \int_{m-1}^m 1(t \leq a_n) P(X = a_n) \, dt \\
    &= \sum_{m=1}^\infty \sum_{n=1}^\infty \int_{m-1}^m 1(t \leq a_n) P(X \geq a_n) \, dt \\
    &= \sum_{m=1}^\infty \lim_{N \to \infty} \sum_{n=1}^N \int_{m-1}^m 1(t \leq a_n) P(X \geq a_n) \, dt \\
    &= \sum_{m=1}^\infty \lim_{N \to \infty} \int_{m-1}^m \left( \sum_{n=1}^N 1(t \leq a_n) P(X \geq a_n) \right) \, dt \\
    &= \sum_{m=1}^\infty \int_{m-1}^m \left( \lim_{N \to \infty} \sum_{n=1}^N 1(t \leq a_n) P(X \geq a_n) \right) \, dt \\
    &= \int_0^\infty \left(\sum_{n=1}^\infty 1(t \leq a_n) P(X = a_n)\right) \, dt,
\end{align*}
which we conclude by removing the auxiliary summations, since it is enough to know that $\sum_{n=1}^N 1(t \leq a_n) P(X = a_n) \to \sum_{n=1}^\infty 1(t \leq a_n) P(X = a_n)$ uniformly in $t \in [m-1, m]$; i.e.,
\[ \abs{\sum_{n=N+1}^\infty 1(t \leq a_n) P(X = a_n)} \leq \sum_{n=N+1}^\infty P(X = a_n) \to 0 \]
as $N \to \infty$ as per earlier (since the tail goes to $0$).

\newpage
We also briefly went over examples multinomial distributions at the end of class;
\begin{itemize}
    \item Suppose $X_1, \dots, X_n$ are independent, and let $P(X_i = j) = p_j$ for $j = 1, \dots, k$. Let $\Omega = \{n_1, \dots, n_k), n_j \geq 0, n_1 + \dots + n_k = n\}$ (read: $k$-sided dice rolled $n$ times, where $n_j$ denotes the number of times $j$ came up). Then
    \[ P((n_1, \dots, n_k)) = \binom{n}{n_1, \dots, n_k} p_1^{n_1} p_2^{n_2} \dots p_k^{n_k}. \]
    \item The geometric distribution; let $0 < p < 1$ denote the probability of getting a head, and let us toss a coin until we get a heads. Let the outcome of $X$ denote the number of tosses it took. Then
    \[ P(X = n) = (1 - p)^{n-1} p, \]
    and we may check $\sum_{n=1}^\infty P(x = n) = 1$ by geometric series.
    \item The Poisson distribution; let $\lambda > 0$. Then
    \[ P(X = n) = \frac{\lambda^n}{n!} e^{-\lambda} \]
    for $n = 0, 1, 2, \dots$.
\end{itemize}
